{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLTK_Tutorial.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNQihjyqunT2ySInKqClQ/s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kkmjkim/nlp-bible-code-2021/blob/master/tutorials/NLTK_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kJsAguTNLbi"
      },
      "source": [
        "##NLTK tutorial link: https://www.youtube.com/playlist?list=PLQVvvaa0QuDf2JswnfiGkliBInZnIC4HL "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmV237nw5VN7"
      },
      "source": [
        "python - only language that has api, module for nlp </br>\n",
        "https://www.nltk.org/ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JJUokVqNPxv",
        "outputId": "5be4b7a3-3513-468b-abf8-9eee2e14708e"
      },
      "source": [
        "!pip3 install nltk==3.5 # 2021-03-20 latest version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nltk==3.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/75/ce35194d8e3022203cca0d2f896dbb88689f9b3fce8e9f9cff942913519d/nltk-3.5.zip (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 5.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk==3.5) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk==3.5) (1.0.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from nltk==3.5) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk==3.5) (4.41.1)\n",
            "Building wheels for collected packages: nltk\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.5-cp37-none-any.whl size=1434674 sha256=e8f7f00e86d4f3adf5b44cefa1530c444fff4a1118a8083ec321a593923600bd\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/8c/3f/b1fe0ba04555b08b57ab52ab7f86023639a526d8bc8d384306\n",
            "Successfully built nltk\n",
            "Installing collected packages: nltk\n",
            "  Found existing installation: nltk 3.3\n",
            "    Uninstalling nltk-3.3:\n",
            "      Successfully uninstalled nltk-3.3\n",
            "Successfully installed nltk-3.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYqzTfzF6ttf"
      },
      "source": [
        "#Tokenizing Words and Sentences (lec. 01) </br>\n",
        "- tokenizing - word / sentence ..\n",
        "- lexicon -  words and meanings, considers context (like a dictionary)\n",
        "- corporas - body of text (Engish language, medical journals ..)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRftACPX6-wo",
        "outputId": "19498d70-8488-4998-c20e-6825cb25d6b9"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FcGtQjB-tzl",
        "outputId": "93e7913b-dda1-4819-df5e-ee18f9f68e96"
      },
      "source": [
        "example_text = \"Hello Mrs. Smith, how are you doing today? The weather is great and Python is awesome. The sky is pinkish-blue.\"\n",
        "print(sent_tokenize(example_text))\n",
        "NLTK’s recommended sentence tokenizer (currently PunktSentenceTokenizer for the specified language)print(word_tokenize(example_text)) # 'Mr.' not 'Mr', '.' -> good!"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Hello Mrs. Smith, how are you doing today?', 'The weather is great and Python is awesome.', 'The sky is pinkish-blue.']\n",
            "['Hello', 'Mrs.', 'Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', 'and', 'Python', 'is', 'awesome', '.', 'The', 'sky', 'is', 'pinkish-blue', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWH-YxEnBCQ-"
      },
      "source": [
        "NLTK’s recommended sentence tokenizer (currently PunktSentenceTokenizer for the specified language)\n",
        "\n",
        "PunktSentenceTokenizer: A sentence tokenizer which uses an **unsupervised algorithm** to build a model for abbreviation words, collocations, and words that start sentences; and then uses that model to find sentence boundaries. This approach has been shown to work well for many European languages.\n",
        "\n",
        "https://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.punkt.PunktSentenceTokenizer "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llxnxg5bDFnk"
      },
      "source": [
        "#Stop Words (lec. 02)\n",
        "\n",
        "words that you want to pull out\n",
        "'a', 'the' -> not meaningful words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwaxaVPTCoJV",
        "outputId": "1845aea6-cb61-4384-ce9e-7de313231c80"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download(\"stopwords\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NX2UVbAGYik",
        "outputId": "051560ba-501e-4ba2-d241-5060671fffbc"
      },
      "source": [
        "example_sentence = \"This is an example showing off stop word filtration.\"\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "print(stop_words) # 179 words"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "179\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L92UynncRQy9"
      },
      "source": [
        "#WordNet (lec. 10)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4RXLvnyNr-J"
      },
      "source": [
        "from nltk.corpus import wordnet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVR4N47INr7q",
        "outputId": "26fc4732-bb6e-4366-e5da-873d69b4dc05"
      },
      "source": [
        "syns = wordnet.synsets(\"program\")\n",
        "print(syns)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Synset('plan.n.01'), Synset('program.n.02'), Synset('broadcast.n.02'), Synset('platform.n.02'), Synset('program.n.05'), Synset('course_of_study.n.01'), Synset('program.n.07'), Synset('program.n.08'), Synset('program.v.01'), Synset('program.v.02')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oah2v8ANr54",
        "outputId": "eaeb0256-e4ef-4c60-e209-bdd85d178231"
      },
      "source": [
        "print(syns[0].name()) # plan.n.01"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "plan.n.01\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NeiakEfwNr4O",
        "outputId": "4fe4a2ad-aa8e-44fd-9ebd-21413bf38f5e"
      },
      "source": [
        "print(syns[0].lemmas())\n",
        "print(syns[0].lemmas()[1].name()) # program"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Lemma('plan.n.01.plan'), Lemma('plan.n.01.program'), Lemma('plan.n.01.programme')]\n",
            "program\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PH6hqjeWaoFi"
      },
      "source": [
        "* In British English, \"programme\" is used for agenda, TV show, or collection of projects (does it even matter here?)\n",
        "\n",
        "* lemmatization: doing \"proper\" reduction to dictionary word form(lemma)\n",
        "\n",
        "* lemma: base form"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADzPSD_Fp5gw",
        "outputId": "4914c674-92e3-44ef-99fe-1780ea7de582"
      },
      "source": [
        "# definition\n",
        "print(syns[0].definition())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a series of steps to be carried out or goals to be accomplished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_thcAeWnNr0S",
        "outputId": "af0ae73b-1fcf-412d-806c-edf4a86e11a3"
      },
      "source": [
        "# examples\n",
        "print(syns[0].examples())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['they drew up a six-step plan', 'they discussed plans for a new bond issue']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvsldZy6cypL",
        "outputId": "9db50e98-01bd-451c-ae85-2469c37110fd"
      },
      "source": [
        "print(wordnet.synsets(\"good\"))\n",
        "print(wordnet.synsets(\"good\")[2].lemmas())\n",
        "print(wordnet.synsets(\"good\")[3].lemmas())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Synset('good.n.01'), Synset('good.n.02'), Synset('good.n.03'), Synset('commodity.n.01'), Synset('good.a.01'), Synset('full.s.06'), Synset('good.a.03'), Synset('estimable.s.02'), Synset('beneficial.s.01'), Synset('good.s.06'), Synset('good.s.07'), Synset('adept.s.01'), Synset('good.s.09'), Synset('dear.s.02'), Synset('dependable.s.04'), Synset('good.s.12'), Synset('good.s.13'), Synset('effective.s.04'), Synset('good.s.15'), Synset('good.s.16'), Synset('good.s.17'), Synset('good.s.18'), Synset('good.s.19'), Synset('good.s.20'), Synset('good.s.21'), Synset('well.r.01'), Synset('thoroughly.r.02')]\n",
            "[Lemma('good.n.03.good'), Lemma('good.n.03.goodness')]\n",
            "[Lemma('commodity.n.01.commodity'), Lemma('commodity.n.01.trade_good'), Lemma('commodity.n.01.good')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2Jp-5KcNryf"
      },
      "source": [
        "# synonyms, antonyms\n",
        "synonyms = []\n",
        "antonyms = []\n",
        "\n",
        "for syn in wordnet.synsets(\"good\"): # [Synset('good.n.01'), Synset('good.n.02'), ...\n",
        "  for l in syn.lemmas(): # ex) l = synsets[0].lemmas()\n",
        "    print(\"l= \", l)\n",
        "    synonyms.append(l.name()) # just the word\n",
        "    print(synonyms)\n",
        "    if l.antonyms(): # if exists\n",
        "      antonyms.append(l.antonyms()[0].name())\n",
        "      print(antonyms)\n",
        "print(set(synonyms))\n",
        "print(set(antonyms))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YfXlRKINrwm",
        "outputId": "42357bb8-d64c-453d-de82-657be7560362"
      },
      "source": [
        "w1 = wordnet.synset(\"ship.n.01\")\n",
        "w2 = wordnet.synset(\"boat.n.01\")\n",
        "print(w1.wup_similarity(w2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9090909090909091\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGuYwJmffZR5",
        "outputId": "358487b9-3a6a-45ef-cb7f-55d303c1adcd"
      },
      "source": [
        "w1 = wordnet.synset(\"ship.n.01\")\n",
        "w2 = wordnet.synset(\"car.n.01\")\n",
        "print(w1.wup_similarity(w2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6956521739130435\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GeB3PCHrfbYl",
        "outputId": "fdcd80d5-9ba3-44de-cd56-8a7ac02aaca8"
      },
      "source": [
        "w1 = wordnet.synset(\"ship.n.01\")\n",
        "w2 = wordnet.synset(\"cat.n.01\")\n",
        "print(w1.wup_similarity(w2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOdrQ6FLfFsG"
      },
      "source": [
        "* wup_similarity(): Wu-Palmer Similarity\n",
        "* Return a score denoting how similar two word senses are, based on the depth of the two senses in the taxonomy and that of their Least Common Subsumer (most specific ancestor node). Note that at this time the scores given do _not_ always agree with those given by Pedersen's Perl implementation of Wordnet Similarity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvGkWtNai0V7",
        "outputId": "155701c9-1c42-44b8-a32e-2ef3b8ebe149"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jx0WQ8UFkF64",
        "outputId": "ce261dbe-3d17-44c7-8b64-71b3aa7752cf"
      },
      "source": [
        "# union\n",
        "s1 = word_tokenize(\"Hello, my name is Kelly. What is your name?\")\n",
        "s2 = word_tokenize(\"Please shut up, Kelly.\")\n",
        "print(\"s1 : \", s1)\n",
        "print(\"s2 : \", s2)\n",
        "s1 = set(s1)\n",
        "s2 = set(s2)\n",
        "s1.union(s2)\n",
        "print(\"set(s1) : \", s1)\n",
        "s1 = s1.union(s2)\n",
        "print(\"s1 union s2 : \", s1.union(s2)) # not redundant"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "s1 :  ['Hello', ',', 'my', 'name', 'is', 'Kelly', '.', 'What', 'is', 'your', 'name', '?']\n",
            "s2 :  ['Please', 'shut', 'up', ',', 'Kelly', '.']\n",
            "set(s1) :  {'Kelly', 'Hello', '.', 'is', 'What', '?', 'my', 'your', 'name', ','}\n",
            "s1 union s2 :  {'Kelly', 'Hello', 'up', 'is', '?', 'my', 'your', 'Please', '.', 'shut', 'What', 'name', ','}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7CsKeC2Nrst",
        "outputId": "2b0d51a6-b7ee-486a-8b30-42958264d9e4"
      },
      "source": [
        "# difference\n",
        "s1 = word_tokenize(\"Hello, my name is Kelly. What is your name?\")\n",
        "s2 = word_tokenize(\"Please shut up, Kelly.\")\n",
        "s1 = set(s1)\n",
        "s2 = set(s2)\n",
        "print(s1.difference(s2))\n",
        "print(s2.difference(s1))\n",
        "print(s1.intersection(s2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'Hello', 'is', 'What', '?', 'my', 'your', 'name'}\n",
            "{'shut', 'Please', 'up'}\n",
            "{'Kelly', '.', ','}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCPnm1hrpn71"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3Bo8NNfxOQs"
      },
      "source": [
        "# Chunking (lec. 5)\n",
        "\n",
        "- group words into meaningful chunks ex) noun phrases\n",
        "- to chunk, combine the POS tags with regular expressions\n",
        "\n",
        "\n",
        "downside: regular expression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-v8JrEIeNrqe"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import state_union\n",
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "from nltk import word_tokenize\n",
        "from IPython.display import display"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f82a5y4J5sLs",
        "outputId": "59701de2-c816-4629-9c33-5718991a7ce2"
      },
      "source": [
        "nltk.download('state_union')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]   Package state_union is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kh6cO_Gk2rg8"
      },
      "source": [
        "Penn Treebank POS Tags\n",
        "\n",
        "1.\tCC\tCoordinating conjunction\n",
        "2.\tCD\tCardinal number\n",
        "3.\tDT\tDeterminer\n",
        "4.\tEX\tExistential there\n",
        "5.\tFW\tForeign word\n",
        "6.\tIN\tPreposition or subordinating conjunction\n",
        "7.\tJJ\tAdjective\n",
        "8.\tJJR\tAdjective, comparative\n",
        "9.\tJJS\tAdjective, superlative\n",
        "10.\tLS\tList item marker\n",
        "11.\tMD\tModal\n",
        "12.\tNN\tNoun, singular or mass\n",
        "13.\tNNS\tNoun, plural\n",
        "14.\tNNP\tProper noun, singular\n",
        "15.\tNNPS\tProper noun, plural\n",
        "16.\tPDT\tPredeterminer\n",
        "17.\tPOS\tPossessive ending\n",
        "18.\tPRP\tPersonal pronoun\n",
        "19.\tPRP\\$\tPossessive pronoun\n",
        "20.\tRB\tAdverb\n",
        "21.\tRBR\tAdverb, comparative\n",
        "22.\tRBS\tAdverb, superlative\n",
        "23.\tRP\tParticle\n",
        "24.\tSYM\tSymbol\n",
        "25.\tTO\tto\n",
        "26.\tUH\tInterjection\n",
        "27.\tVB\tVerb, base form\n",
        "28.\tVBD\tVerb, past tense\n",
        "29.\tVBG\tVerb, gerund or present participle\n",
        "30.\tVBN\tVerb, past participle\n",
        "31.\tVBP\tVerb, non-3rd person singular present\n",
        "32.\tVBZ\tVerb, 3rd person singular present\n",
        "33.\tWDT\tWh-determiner\n",
        "34.\tWP\tWh-pronoun\n",
        "35.\tWP$\tPossessive wh-pronoun\n",
        "36.\tWRB\tWh-adverb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "QniraSO12rSu",
        "outputId": "2df9cd70-d7ff-4b12-bdc5-e2a28cc53d7d"
      },
      "source": [
        "# https://dbrang.tistory.com/1194 \n",
        "\n",
        "sent = 'Prime Minister Boris Johnson had previously said the UK would leave by 31 October.\"\n",
        "\n",
        "word_token = word_tokenize(sent)\n",
        "pos_tags = nltk.pos_tag(word_token)\n",
        "\n",
        "print(\"sentence: \\n\", sent)\n",
        "print(\"word token: \\n\", word_token)\n",
        "\n",
        "\n",
        "chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
        "chunkParser = nltk.RegexpParser(chunkGram)\n",
        "chunked = chunkParser.parser(pos_tags)\n",
        "# chunked = chunkParser.__str__()\n",
        "\n",
        "\n",
        "# print(chunked)       \n",
        "display(chunked) # failed to display it on jupyter notebook\n",
        "# chunked.draw()     \n",
        "\n",
        "# https://pythonprogramming.net/chunking-nltk-tutorial/\n",
        "\n",
        "# train_text = state_union.raw(\"2005-GWBush.txt\")\n",
        "# sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
        "\n",
        "# custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
        "# tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
        "\n",
        "# def process_content():\n",
        "#     try:\n",
        "#         for i in tokenized:\n",
        "#             words = nltk.word_tokenize(i)\n",
        "#             tagged = nltk.pos_tag(words)\n",
        "\n",
        "#             # r: regex / .: any char, except for a new line / RB: adverb\n",
        "#             chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
        "#             chunkParser = nltk.RegexpParser(chunkGram)\n",
        "#             chunked = chunkParser.parse(tagged)\n",
        "            \n",
        "#             chunked.draw()     \n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(str(e))\n",
        "\n",
        "# process_content()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sentence: \n",
            " Prime Minister Boris Johnson had previously said the UK would leave by 31 October.\n",
            "word token: \n",
            " ['Prime', 'Minister', 'Boris', 'Johnson', 'had', 'previously', 'said', 'the', 'UK', 'would', 'leave', 'by', '31', 'October', '.']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"chunk.RegexpParser with 1 stages:\\nRegexpChunkParser with 1 rules:\\n       <ChunkRule: '<RB.?>*<VB.?>*<NNP>+<NN>?'>\""
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlPUW84yNrok"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-bqHmGsNrm_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSgx8_83NrW0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}