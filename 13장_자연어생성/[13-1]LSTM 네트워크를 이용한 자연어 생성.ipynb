{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "13장_자연어생성.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zo-AfU3Fkuga"
      },
      "source": [
        "LSTM 네트워크를 이용한 자연어 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAijyNLb0EQ9"
      },
      "source": [
        "from __future__ import print_function\n",
        "from tensorflow.keras.callbacks import LambdaCallback\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.utils import get_file\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "import io"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CvPjpwZ0K4m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45d144b3-65e4-4989-862a-c2cc7b8307ac"
      },
      "source": [
        "# 텍스트 파일 불러오기\n",
        "fpath = get_file(\n",
        "  'nietzsche.txt',\n",
        "  origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
        "with io.open(fpath, encoding='utf-8') as f:\n",
        "  text = f.read().lower()\n",
        "\n",
        "print(\"text size:\", len(text)) # 600893 letters\n",
        "\n",
        "# 어휘 사전 생성\n",
        "chars = sorted(list(set(text)))\n",
        "print(\"chars size:\", len(chars), chars[0:5]) # 57\n",
        "char2index = dict((c, i) for i, c in enumerate(chars))\n",
        "index2char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "# 음절 단위 학습 데이터 생성\n",
        "maxlen, step = 40, 3 # 40-gram, 3 steps\n",
        "sentences, next_chars = [], []\n",
        "for i in range(0, len(text) - maxlen, step): # 0 ~ 600853 / 0, 3, 6 .. 600852\n",
        "  sentences.append(text[i : i + maxlen]) # ex) [0] ~ [39]\n",
        "  next_chars.append(text[i + maxlen])  # labels (=which char should come next?) ex) [40]n, [41]-, [42]-\n",
        "\n",
        "print ('The number of sentences:', len(sentences)) # 200285 (=600853/3steps 올림)\n",
        "print(\"sentence examples:\", sentences[0:4])\n",
        "\n",
        "# sparse\n",
        "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool) # (20만, 40, 57)\n",
        "y = np.zeros((len(sentences), len(chars)), dtype=np.bool) # (20만, 57)\n",
        "\n",
        "for i, sentence in enumerate(sentences):\n",
        "  for t, char in enumerate(sentence):\n",
        "    x[i, t, char2index[char]] = 1 # input\n",
        "  y[i, char2index[next_chars[i]]] = 1 # label"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/nietzsche.txt\n",
            "606208/600901 [==============================] - 0s 0us/step\n",
            "text size: 600893\n",
            "chars size: 57 ['\\n', ' ', '!', '\"', \"'\"]\n",
            "The number of sentences: 200285\n",
            "sentence examples: ['preface\\n\\n\\nsupposing that truth is a woma', 'face\\n\\n\\nsupposing that truth is a woman--', 'e\\n\\n\\nsupposing that truth is a woman--wha', '\\nsupposing that truth is a woman--what t']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43cwZI1v0Mj-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5d58ceb-b8a3-42c2-aa42-3941833de55f"
      },
      "source": [
        "# 딥러닝 모델 선언\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(maxlen, len(chars)))) # 128: output, (40, 57): (time_steps, features)\n",
        "model.add(Dense(len(chars), activation='softmax')) # 57\n",
        "optimizer = RMSprop(learning_rate=0.01 )\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
        "\n",
        "\n",
        "# 입력된 확률값에 따른 다음 음절 샘플링\n",
        "def sample(preds, temperature=1.0):\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "# 1회 (1 epoch) 학습\n",
        "def on_epoch_end(epoch, _): # called at the end of every epoch.\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    start_index = random.randint(0, len(text) - maxlen - 1 ) # (0, 600893-40-1) = (0, 600852)\n",
        "    for diversity in [0.2, 0.5, 1.0, 1.2]: # hypermarameters\n",
        "        print('\\nDiversity:', diversity)\n",
        "        generated = ''\n",
        "        sentence = text[start_index : start_index + maxlen]\n",
        "        generated += sentence\n",
        "        print('Seed: %s' % sentence)\n",
        "        sys.stdout.write(generated)\n",
        "        for i in range (400): # generate 400 more characters\n",
        "            x_pred = np.zeros((1, maxlen, len(chars))) # (1, 40, 57) -> dimension??\n",
        "            for t, char in enumerate(sentence):\n",
        "                x_pred[0, t, char2index[char]] = 1.\n",
        "            preds = model.predict(x_pred, verbose=0)[0] # C-E loss for 57 chars; preds.shape: (1, 57)\n",
        "            next_index = sample(preds, diversity)\n",
        "            next_char = index2char[next_index]\n",
        "            sentence = sentence[1:] + next_char # shift (ngram)\n",
        "            sys.stdout.write(next_char)\n",
        "            sys.stdout.flush() # stdout에 쌓여 있는 버퍼를 강제로 뱉어내어 터미널에 출력되도록 한다고 생각하면 됨\n",
        "\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
        "model.fit(x, y,\n",
        "          batch_size=128,\n",
        "          epochs=60,\n",
        "          callbacks=[print_callback]) # 실습은 epoch 더 작게 하기"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "of aspucanis cruelty, this brects and serautorous, boan--will, thereby honefcervainally of\n",
            "ourselunt\n",
            "ettinaurne:s-trough emotehercate\n",
            "live judge to regard now: the onespeverted floEpoch 9/60\n",
            "1464/1565 [===========================>..] - ETA: 0s - loss: 1.3706"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}